<div align="center">

<h1>ğŸ¤– Emotion-Reactive Desk Assistant Robot</h1>

<p><strong>A real-time AI-powered robotic arm that sees your face, reads your emotions, and physically reacts â€” tracking you with its body and expressing feelings through movement.</strong></p>

<br/>

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Arduino](https://img.shields.io/badge/Arduino-Uno-00979D?style=for-the-badge&logo=arduino&logoColor=white)
![YOLOv8](https://img.shields.io/badge/YOLOv8-Face%20Detection-FF6B6B?style=for-the-badge)
![DeepFace](https://img.shields.io/badge/DeepFace-Emotion%20AI-9B59B6?style=for-the-badge)
![OpenCV](https://img.shields.io/badge/OpenCV-Vision-5C3317?style=for-the-badge&logo=opencv&logoColor=white)

<br/>

> *"A robot that doesn't just move â€” it understands how you feel."*

</div>

---

## ğŸŒŸ What Is This?

This project is a **Semester 4 Robotics Project** â€” a 3-servo robotic arm controlled by a **computer vision + emotion AI pipeline**. It connects a live webcam feed to an Arduino Uno over USB serial.

Here's what happens in real time:
- ğŸ‘ï¸ The camera sees your face
- ğŸ§  AI detects your emotion (happy, sad, angry, surprised, neutral, bored, excited)
- ğŸ¤– The robot arm **physically tracks your face** left/right and up/down
- ğŸ’ª The **elbow joint expresses your emotion** with a specific pose
- ğŸ“Ÿ An OLED display on the robot shows your current emotion label

---

## ğŸ¥ Demo

```
Webcam â†’ YOLO Face Detect â†’ DeepFace Emotion â†’ Filter + State Machine â†’ Arduino Serial â†’ 3 Servos Move
```

| Emotion    | Elbow Angle | Behaviour                         |
|------------|-------------|-----------------------------------|
| ğŸ˜Š HAPPY   | 130Â°        | Arm raised high â€” celebratory     |
| ğŸ˜¢ SAD     | 60Â°         | Arm drooped down â€” dejected       |
| ğŸ˜¡ ANGRY   | 40Â°         | Arm pulled tight â€” tense          |
| ğŸ˜² SURPRISE| 150Â°        | Arm fully extended â€” shocked      |
| ğŸ˜ NEUTRAL | 90Â°         | Arm at rest â€” relaxed             |
| ğŸ˜´ BORED   | 80Â°         | Arm slightly drooped â€” disengaged |
| ğŸ¤© EXCITED | 130Â°        | Same as happy â€” energetic         |

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Python (PC Side)                         â”‚
â”‚                                                                  â”‚
â”‚  Webcam â”€â”€â–º YOLOv8n-face â”€â”€â–º DeepFace â”€â”€â–º EmotionFilter        â”‚
â”‚                  â”‚                              â”‚                â”‚
â”‚            [face x,y pos]               DerivedEmotionEngine    â”‚
â”‚                  â”‚                   (EXCITED / BORED logic)    â”‚
â”‚                  â”‚                              â”‚                â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º EmotionStateMachine               â”‚
â”‚                                  (voting window)                 â”‚
â”‚                                       â”‚                          â”‚
â”‚                              TrackerController                   â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                    [B:xx S:xx E:xx]          [EMO:xxx]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ USB Serial (115200 baud)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Arduino Uno (Hardware)                      â”‚
â”‚                                                                  â”‚
â”‚   Parse "B:90 S:20 E:130"         Parse "EMO:HAPPY"            â”‚
â”‚          â”‚                                 â”‚                     â”‚
â”‚   moveSmooth() on 3 servos         Display on OLED              â”‚
â”‚   (fast â†’ slow deceleration)                                    â”‚
â”‚                                                                  â”‚
â”‚   D9 â†’ Base Servo (left/right)                                  â”‚
â”‚   D10 â†’ Shoulder Servo (up/down)                                â”‚
â”‚   D11 â†’ Elbow Servo (emotion pose)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
robo-sem4/
â”‚
â”œâ”€â”€ README.md
â”‚
â””â”€â”€ software/
    â”‚
    â”œâ”€â”€ main_track.py              â† ğŸš€ MAIN ENTRY POINT â€” runs everything
    â”‚
    â”œâ”€â”€ models/
    â”‚   â””â”€â”€ yolov8n-face.pt        â† Pre-trained YOLO face detection model
    â”‚
    â”œâ”€â”€ vision/                    â† ğŸ‘ï¸  Computer Vision Layer
    â”‚   â”œâ”€â”€ emotion_detector.py    â† DeepFace wrapper for emotion inference
    â”‚   â”œâ”€â”€ face_detector.py       â† Standalone YOLO face detection test
    â”‚   â””â”€â”€ test.py                â† Integration test (without tracking)
    â”‚
    â”œâ”€â”€ perception/                â† ğŸ§¹ Signal Filtering Layer
    â”‚   â””â”€â”€ filters.py             â† EmotionFilter (confidence + dominance gating)
    â”‚
    â”œâ”€â”€ emotion_engine/            â† ğŸ§  Emotion Intelligence Layer
    â”‚   â”œâ”€â”€ derived.py             â† Custom EXCITED + BORED state detection
    â”‚   â””â”€â”€ state_machine.py       â† Voting-based stable emotion state machine
    â”‚
    â””â”€â”€ control/                   â† ğŸ¦¾ Robot Control Layer
        â”œâ”€â”€ tracker_controller.py  â† Main controller (tracking + emotion â†’ angles)
        â””â”€â”€ emotion_to_servo.py    â† Manual pose test script for Arduino
```

---

## ğŸ§© Module Deep Dive

### ğŸ‘ï¸ Vision Layer

#### `vision/emotion_detector.py` â€” EmotionDetector
Wraps **DeepFace** to classify emotions from a face crop. Since YOLO already isolated the face, DeepFace is told to skip its own detection (`detector_backend="skip"`) â€” this makes it significantly faster.

```python
detector = EmotionDetector()
emotions = detector.predict(face_crop)
# â†’ {"happy": 0.82, "neutral": 0.10, "sad": 0.04, ...}
```

---

### ğŸ§¹ Perception Layer

#### `perception/filters.py` â€” EmotionFilter
Raw emotion scores are noisy and jittery. This filter applies **3 gates** before accepting a reading:

| Gate | Rule |
|------|------|
| **Confidence Gate** | Top emotion must score â‰¥ 55% |
| **Neutral Suppression** | If neutral wins by < 15%, prefer the runner-up |
| **Dominance Gate** | Top emotion must beat 2nd by â‰¥ 15% margin |

If any gate fails â†’ the frame is thrown out (returns `None`).

---

### ğŸ§  Emotion Engine

#### `emotion_engine/derived.py` â€” DerivedEmotionEngine
DeepFace only has 7 basic emotions. We added **2 custom derived states**:

| Derived Emotion | Trigger Condition |
|-----------------|-------------------|
| **EXCITED** | `happy > 60%` AND `surprise > 20%` simultaneously |
| **BORED** | Person is `neutral > 90%` for **5+ continuous seconds** |

Derived emotions take **priority** over base emotions.

#### `emotion_engine/state_machine.py` â€” EmotionStateMachine
Even after filtering, an emotion can flicker. The state machine uses **majority voting** on a rolling window:

```
Window: [HAPPY, HAPPY, NEUTRAL, HAPPY, HAPPY, HAPPY, HAPPY, HAPPY, HAPPY, HAPPY]
Threshold: 7 / 12 votes â†’ HAPPY wins â†’ robot switches to HAPPY pose
```
This makes the robot feel **calm, deliberate, and confident** in its reactions.

---

### ğŸ¦¾ Control Layer

#### `control/tracker_controller.py` â€” TrackerController
The heart of the robot. Handles:

**1. Face Tracking (Base + Shoulder)**
```
error_x = face_center_x - screen_center_x
base_angle    = 90  +  error_x * 0.20   (rotate toward face)
shoulder_angle  = 20  -  error_y * 0.16   (tilt toward face)
```
A **10-pixel deadzone** prevents micro-jitter when your face is nearly centered.

**2. Emotion â†’ Elbow Angle lookup table**

**3. Serial Protocol**
```
B:90 S:20 E:130\n   â† Servo command
EMO:HAPPY\n         â† OLED display command
```
Duplicate commands are suppressed â€” the Arduino isn't spammed every frame.

---

## âš¡ Arduino Firmware

The Arduino runs a smooth motion algorithm:

```cpp
void moveSmooth(int &cur, int target, Servo &s) {
    while (cur != target) {
        int dist = abs(target - cur);
        int step = (dist > SWITCH_ZONE) ? STEP_FAST : STEP_SLOW;
        // Fast when far â†’ Slow as it approaches
        cur += (target > cur) ? step : -step;
        s.write(cur);
        delay(10);
    }
}
```

- **Fast phase**: 4Â° steps when far from target
- **Slow phase**: 1Â° steps when within 15Â° of target
- Creates a **natural deceleration** feel â€” not robotic and jerky

**Serial command format:**
```
B:90 S:20 E:120\n    â†’ moves all 3 servos
EMO:HAPPY\n          â†’ updates OLED emotion display
```

---

## ğŸ”Œ Hardware Setup

### Components

| Component | Details |
|-----------|---------|
| Microcontroller | Arduino Uno |
| Servos (Ã—3) | Standard hobby servos (SG90 or MG996R) |
| Camera | USB Webcam or built-in laptop camera |
| Display (optional) | I2C OLED (128Ã—64) |
| Power | External 5V supply for servos (don't power from Arduino 5V!) |

### Servo Wiring

| Servo | Arduino Pin | Movement | Angle Range |
|-------|------------|----------|-------------|
| Base  | D9  | Left â†” Right (tracks face) | 60Â° â€“ 120Â° |
| Shoulder | D10 | Up â†” Down (tracks face) | 0Â° â€“ 60Â° |
| Elbow | D11 | Emotion expression | 30Â° â€“ 150Â° |

### Serial Connection
```
PC (Python) â”€â”€â”€â”€ USB â”€â”€â”€â”€ Arduino Uno (COM port)
Baud Rate: 115200
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- Arduino IDE
- Arduino Uno connected via USB

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/robo-sem4.git
cd robo-sem4
```

### 2. Create a Virtual Environment

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

### 3. Install Python Dependencies

```bash
pip install ultralytics deepface opencv-python torch pyserial
```

### 4. Upload Arduino Firmware

1. Open **Arduino IDE**
2. Load the sketch from the `notes` file (copy the `#include <Servo.h>` code block)
3. Select your board: `Tools â†’ Board â†’ Arduino Uno`
4. Select your port: `Tools â†’ Port â†’ COMx`
5. Click **Upload**

### 5. Configure the Serial Port

In `software/main_track.py`, update the COM port to match your Arduino:
```python
controller = TrackerController(port="COM8")  # â† Change this to your port
```

### 6. Run the Project

```bash
cd software
python main_track.py
```

A window titled **"Desk Assistant Robot"** will open. Point the camera at your face and watch the arm follow you and react to your emotions!

---

## ğŸ® Controls

| Key | Action |
|-----|--------|
| `Q` | Quit the application cleanly (robot returns to neutral) |
| Close window | Same as pressing Q |

---

## ğŸ§ª Testing Individual Components

Test each layer independently before running the full system:

```bash
# Test ONLY face detection (no emotion, no arm)
python software/vision/face_detector.py

# Test emotion + arm WITHOUT face tracking
python software/vision/test.py

# Manually send poses to Arduino (quick hardware test)
python software/control/emotion_to_servo.py
```

---

## ğŸ› ï¸ Configuration & Tuning

All key parameters are in `main_track.py` and `tracker_controller.py`:

```python
# How aggressively the arm follows your face
TRACK_GAIN_X = 0.20   # left/right sensitivity
TRACK_GAIN_Y = 0.16   # up/down sensitivity

# Ignore face movements smaller than this (prevents jitter)
DEADZONE = 10          # pixels

# Emotion filter thresholds
EmotionFilter(
    window_size=12,        # frames to consider
    min_confidence=0.55,   # minimum score to count
    dominance_margin=0.15  # must beat runner-up by this much
)

# How long neutral = BORED
DerivedEmotionEngine(bored_time_sec=5.0)

# State machine stability
EmotionStateMachine(window_size=12, threshold=7)
# â†’ emotion needs 7/12 votes to take effect
```

---

## ğŸ§ª Tech Stack

| Technology | Role |
|------------|------|
| **YOLOv8n-face** | Real-time face detection (Ultralytics) |
| **DeepFace** | Emotion classification from face crops |
| **OpenCV** | Webcam capture, image processing, display |
| **PyTorch** | GPU acceleration (CUDA if available) |
| **PySerial** | USB serial communication to Arduino |
| **Arduino + Servo.h** | Hardware servo motor control |

---

## ğŸ“Š Performance

- **Face Detection**: ~30+ FPS on modern hardware (YOLO is very fast)
- **Emotion Detection**: ~5â€“10 FPS (DeepFace is heavier â€” runs every frame but is the bottleneck)
- **Serial Latency**: ~2ms (115200 baud, negligible)
- **Motion Smoothing**: Handled purely by Arduino firmware (no Python overhead)
- **GPU**: CUDA is used automatically if an NVIDIA GPU is present

---

## ğŸ“ Design Decisions

> **Why YOLO for faces instead of letting DeepFace detect?**
> YOLO is significantly faster and more accurate for real-time detection. We use `detector_backend="skip"` in DeepFace so it only classifies emotion on the already-cropped face â€” no redundant detection.

> **Why 3 layers of emotion stability (filter â†’ derived â†’ state machine)?**
> Raw emotion readings are extremely noisy frame-to-frame. Without filtering, the robot would look like it's having a seizure. Each layer adds a different kind of stability â€” confidence gating, custom logic, and temporal voting.

> **Why separate base/shoulder from elbow?**
> Clean separation of concerns: 2 joints chase your physical position; 1 joint expresses emotional state. This makes the robot feel like it has a *body* and a *soul*.

---
</div>
