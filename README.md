<div align="center">

<h1>Emotion Aware and Face Tracking Expressive Robotic Arm</h1>

<p><em>A real-time AI-powered robotic arm that sees your face, tracks it, reads your emotion, and physically reacts.</em></p>

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Arduino](https://img.shields.io/badge/Arduino-Uno-00979D?style=for-the-badge&logo=arduino&logoColor=white)
![YOLOv8](https://img.shields.io/badge/YOLOv8-Face%20Detection-FF6B6B?style=for-the-badge)
![DeepFace](https://img.shields.io/badge/DeepFace-Emotion%20AI-9B59B6?style=for-the-badge)
![OpenCV](https://img.shields.io/badge/OpenCV-Vision-5C3317?style=for-the-badge&logo=opencv&logoColor=white)

</div>

---

## What Makes This Special

- ðŸŽ¯ **Real-time face tracking** â€” the arm physically follows your face using two servo joints
- ðŸ§  **Emotion-aware movement** â€” detects 7 emotions and expresses them through a dedicated elbow servo
- ðŸ”— **End-to-end pipeline** â€” from webcam frame to servo pulse in milliseconds
- ðŸ›¡ï¸ **Noise-resilient AI** â€” 3-layer emotion stability pipeline (confidence filtering â†’ derived states â†’ voting state machine) prevents jitter and false triggers
- ðŸ’¡ **Custom derived emotions** â€” goes beyond DeepFace's 7 defaults; detects **EXCITED** and **BORED** using compound logic
- ðŸ“Ÿ **OLED feedback** â€” Arduino displays the detected emotion as text in real time

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| Face Detection | **YOLOv8n-face** (Ultralytics) â€” fast, accurate, GPU-accelerated |
| Emotion Recognition | **DeepFace** â€” skips re-detection since YOLO already crops the face |
| Vision Pipeline | **OpenCV** â€” webcam capture, display, drawing |
| Serial Comms | **PySerial** â€” USB serial at 115200 baud to Arduino Uno |
| Hardware | **Arduino Uno + 3Ã— SG90 Servos** â€” smooth motion firmware on-board |
| Acceleration | **PyTorch / CUDA** â€” automatically uses GPU if available |

---

## Architecture

```
Webcam â†’ YOLOv8 (face box) â†’ DeepFace (raw emotions)
              â”‚                        â”‚
        [x, y position]        EmotionFilter â†’ DerivedEmotionEngine â†’ StateMachine
              â”‚                                                              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TrackerController â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”˜
                                       â”‚
                          USB Serial â†’ Arduino Uno
                                       â”‚
                          Base â”€â”€ Shoulder â”€â”€ Elbow Servos
```

---

## Emotion â†’ Motion Mapping

| Emotion | Elbow Pose | Feel |
|---------|-----------|------|
| ðŸ˜Š HAPPY / ðŸ¤© EXCITED | 130Â° | Raised â€” celebratory |
| ðŸ˜² SURPRISE | 150Â° | Fully extended â€” shocked |
| ðŸ˜ NEUTRAL | 90Â° | Rest position |
| ðŸ˜´ BORED | 80Â° | Slightly drooped |
| ðŸ˜¢ SAD | 60Â° | Drooped down |
| ðŸ˜¡ ANGRY | 40Â° | Pulled tight |

---

## ðŸ“ Project Structure

```
software/
â”œâ”€â”€ main_track.py              â† Entry point â€” runs the full pipeline
â”œâ”€â”€ models/yolov8n-face.pt     â† Pre-trained face detection model
â”œâ”€â”€ vision/
â”‚   â””â”€â”€ emotion_detector.py    â† DeepFace emotion inference wrapper
â”œâ”€â”€ perception/
â”‚   â””â”€â”€ filters.py             â† Confidence + dominance gating filter
â”œâ”€â”€ emotion_engine/
â”‚   â”œâ”€â”€ derived.py             â† EXCITED / BORED compound emotion logic
â”‚   â””â”€â”€ state_machine.py       â† Majority-voting emotion stabiliser
â””â”€â”€ control/
    â””â”€â”€ tracker_controller.py  â† Face tracking + emotion â†’ servo angles
```

---

## Getting Started

```bash
git clone https://github.com/yourusername/Emotion-Aware-and-Face-Tracking-Expressive-Robotic-Arm.git
```

```bash
# 1. Install dependencies
pip install ultralytics deepface opencv-python torch pyserial

# 2. Set your Arduino COM port in main_track.py
controller = TrackerController(port="COM8")

# 3. Upload the Arduino sketch (Servo.h based, see notes file)

# 4. Run
cd software
python main_track.py
```

> Press **`Q`** or close the window to exit cleanly. The arm returns to neutral automatically.

---

## ðŸ“Š Performance

- **~30 FPS** face detection (YOLO, GPU)
- **~5â€“10 FPS** emotion inference (DeepFace â€” the bottleneck)
- **~2ms** serial latency
- Smooth servo motion handled entirely on the Arduino (no Python overhead)

---


